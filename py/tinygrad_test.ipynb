{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a989e4db",
   "metadata": {},
   "source": [
    "## Playground for tinygrad\n",
    "https://github.com/geohot/tinygrad/blob/91a352a8e2697828a4b1eafa2bdc1a9a3b7deffa/tinygrad/tensor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feed4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_mnist():\n",
    "    def fetch(url):\n",
    "        import requests, gzip, os, hashlib, numpy\n",
    "        fp = os.path.join(\"/tmp\", hashlib.md5(url.encode('utf-8')).hexdigest())\n",
    "        if os.path.isfile(fp):\n",
    "            with open(fp, \"rb\") as f:\n",
    "                dat = f.read()\n",
    "        else:\n",
    "            with open(fp, \"wb\") as f:\n",
    "                dat = requests.get(url).content\n",
    "                f.write(dat)\n",
    "        return numpy.frombuffer(gzip.decompress(dat), dtype=numpy.uint8).copy()\n",
    "\n",
    "    X_train = fetch(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\")[0x10:].reshape((-1, 28, 28))\n",
    "    Y_train = fetch(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\")[8:]\n",
    "    X_test = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\")[0x10:].reshape((-1, 28, 28))\n",
    "    Y_test = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\")[8:]\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9232038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by https://github.com/karpathy/micrograd/blob/master/micrograd/engine.py\n",
    "from functools import partialmethod\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# **** start with two base classes ****\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data):\n",
    "        #print(type(data), data)\n",
    "        if type(data) != np.ndarray:\n",
    "            print(\"error constructing tensor with %r\" % data)\n",
    "            assert (False)\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._ctx = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Tensor %r with grad %r\" % (self.data, self.grad)\n",
    "\n",
    "    def backward(self, allow_fill=True):\n",
    "        #print(\"running backward on\", self)\n",
    "        if self._ctx is None:\n",
    "            return\n",
    "\n",
    "        if self.grad is None and allow_fill:\n",
    "            # fill in the first grad with one\n",
    "            # this is \"implicit gradient creation\"\n",
    "            assert self.data.size == 1\n",
    "            self.grad = np.ones_like(self.data)\n",
    "\n",
    "        assert (self.grad is not None)\n",
    "\n",
    "        grads = self._ctx.backward(self._ctx, self.grad)\n",
    "        if len(self._ctx.parents) == 1:\n",
    "            grads = [grads]\n",
    "        for t, g in zip(self._ctx.parents, grads):\n",
    "            if g.shape != t.data.shape:\n",
    "                print(\"grad shape must match tensor shape in %r, %r != %r\" %\n",
    "                      (self._ctx, g.shape, t.data.shape))\n",
    "                assert (False)\n",
    "            t.grad = g\n",
    "            t.backward(False)\n",
    "\n",
    "    def mean(self):\n",
    "        div = Tensor(np.array([1 / self.data.size]))\n",
    "        return self.sum().mul(div)\n",
    "\n",
    "\n",
    "# An instantiation of the Function is the Context\n",
    "class Function:\n",
    "    def __init__(self, *tensors):\n",
    "        self.parents = tensors\n",
    "        self.saved_tensors = []\n",
    "\n",
    "    def save_for_backward(self, *x):\n",
    "        self.saved_tensors.extend(x)\n",
    "\n",
    "    # note that due to how partialmethod works, self and arg are switched\n",
    "    def apply(self, arg, *x):\n",
    "        ctx = arg(self, *x)\n",
    "        ret = Tensor(arg.forward(ctx, self.data, *[t.data for t in x]))\n",
    "        ret._ctx = ctx\n",
    "        return ret\n",
    "\n",
    "\n",
    "def register(name, fxn):\n",
    "    setattr(Tensor, name, partialmethod(fxn.apply, fxn))\n",
    "\n",
    "\n",
    "# **** implement a few functions ****\n",
    "\n",
    "class Mul(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y):\n",
    "        ctx.save_for_backward(x, y)\n",
    "        return x * y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, y = ctx.saved_tensors\n",
    "        return y * grad_output, x * grad_output\n",
    "\n",
    "\n",
    "register('mul', Mul)\n",
    "\n",
    "\n",
    "class Add(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y):\n",
    "        return x + y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, grad_output\n",
    "\n",
    "\n",
    "register('add', Add)\n",
    "\n",
    "\n",
    "class ReLU(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return np.maximum(input, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.copy()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "register('relu', ReLU)\n",
    "\n",
    "\n",
    "class Dot(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight):\n",
    "        ctx.save_for_backward(input, weight)\n",
    "        return input.dot(weight)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight = ctx.saved_tensors\n",
    "        grad_input = grad_output.dot(weight.T)\n",
    "        grad_weight = grad_output.T.dot(input).T\n",
    "        return grad_input, grad_weight\n",
    "\n",
    "\n",
    "register('dot', Dot)\n",
    "\n",
    "\n",
    "class Sum(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return np.array([input.sum()])\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * np.ones_like(input)\n",
    "\n",
    "\n",
    "register('sum', Sum)\n",
    "\n",
    "\n",
    "class LogSoftmax(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        def logsumexp(x):\n",
    "            #return np.log(np.exp(x).sum(axis=1))\n",
    "            c = x.max(axis=1)\n",
    "            return c + np.log(np.exp(x - c.reshape((-1, 1))).sum(axis=1))\n",
    "\n",
    "        output = input - logsumexp(input).reshape((-1, 1))\n",
    "        ctx.save_for_backward(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output, = ctx.saved_tensors\n",
    "        return grad_output - np.exp(output) * grad_output.sum(axis=1).reshape((-1, 1))\n",
    "\n",
    "\n",
    "register('logsoftmax', LogSoftmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5674d2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3614cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def layer_init(m, h):\n",
    "    ret = np.random.uniform(-1., 1., size=(m, h)) / np.sqrt(m * h)\n",
    "    return ret.astype(np.float32)\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, tensors, lr):\n",
    "        self.tensors = tensors\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for t in self.tensors:\n",
    "            t.data -= self.lr * t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "200970c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.02 accuracy 0.95: 100%|██████████| 1000/1000 [00:07<00:00, 135.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy is 0.962500\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# import numpy as np\n",
    "# from tinygrad.tensor import Tensor\n",
    "# from tinygrad.nn import layer_init, SGD\n",
    "# from tinygrad.utils import fetch_mnist\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "# load the mnist dataset\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = fetch_mnist()\n",
    "\n",
    "\n",
    "# train a model\n",
    "\n",
    "class TinyBobNet:\n",
    "    def __init__(self):\n",
    "        self.l1 = Tensor(layer_init(784, 128))\n",
    "        self.l2 = Tensor(layer_init(128, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.dot(self.l1).relu().dot(self.l2).logsoftmax()\n",
    "\n",
    "\n",
    "# optimizer\n",
    "\n",
    "\n",
    "model = TinyBobNet()\n",
    "optim = SGD([model.l1, model.l2], lr=0.01)\n",
    "\n",
    "BS = 128\n",
    "losses, accuracies = [], []\n",
    "for i in (t := trange(1000)):\n",
    "    samp = np.random.randint(0, X_train.shape[0], size=(BS))\n",
    "\n",
    "    x = Tensor(X_train[samp].reshape((-1, 28 * 28)))\n",
    "    Y = Y_train[samp]\n",
    "    y = np.zeros((len(samp), 10), np.float32)\n",
    "    y[range(y.shape[0]), Y] = -1.0\n",
    "    y = Tensor(y)\n",
    "\n",
    "    # network\n",
    "    outs = model.forward(x)\n",
    "\n",
    "    # NLL loss function\n",
    "    loss = outs.mul(y).mean()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    cat = np.argmax(outs.data, axis=1)\n",
    "    accuracy = (cat == Y).mean()\n",
    "\n",
    "    # printing\n",
    "    loss = loss.data\n",
    "    losses.append(loss)\n",
    "    accuracies.append(accuracy)\n",
    "    t.set_description(\"loss %.2f accuracy %.2f\" % (loss, accuracy))\n",
    "\n",
    "\n",
    "# evaluate\n",
    "def numpy_eval():\n",
    "    Y_test_preds_out = model.forward(Tensor(X_test.reshape((-1, 28 * 28))))\n",
    "    Y_test_preds = np.argmax(Y_test_preds_out.data, axis=1)\n",
    "    return (Y_test == Y_test_preds).mean()\n",
    "\n",
    "\n",
    "accuracy = numpy_eval()\n",
    "print(\"test set accuracy is %f\" % accuracy)\n",
    "assert accuracy > 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95810175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.97946676e-03,  1.26960746e-04,  2.38204352e-03, ...,\n         1.62725593e-03, -1.91289769e-03,  5.89614210e-04],\n       [-1.50905410e-03, -8.47915944e-04, -1.29279113e-04, ...,\n         1.60180428e-03, -1.19924149e-03, -3.13649699e-03],\n       [ 2.91572395e-03, -1.64959207e-03,  1.55070925e-03, ...,\n         3.03992396e-03,  2.81632203e-03,  2.72833928e-03],\n       ...,\n       [ 2.09908793e-03,  2.37899576e-03, -1.70894340e-03, ...,\n         9.52412665e-04,  1.39679445e-03, -1.39341236e-03],\n       [ 2.95553752e-03,  1.46551232e-03, -2.74204922e-05, ...,\n        -1.86793460e-03, -2.42592511e-03, -1.90780370e-03],\n       [-1.18676934e-03, -2.62226723e-03, -1.19277436e-04, ...,\n         1.63433055e-04, -1.38105033e-03,  3.04675312e-03]], dtype=float32)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.l1.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
